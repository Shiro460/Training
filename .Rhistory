table(usedcars$model)
table(usedcars$color)
# モデルの1次元表をmodel_tableに定義し、直接構成比を出力
model_table <- table(usedcars$model)
prop.table(model_table)
# 色の直接構成比を%を単位として出力
color_table <- table(usedcars$color)
color_pct <- prop.table(coloar_table)*100
color_pct <- prop.table(color_table)*100
round(color_pct, digit = 1)
# 散布図の作成
plot(x = usedcars$mileage, y = usedcars$price,
main = "Scatterplot of Price vs. Mileage",
xlab = "Used Car Odometer (mi)",
ylab = "Used Car Price ($)")
# 2次元クロス表(分割表)の作成
# 2次元クロス表は、名義変数の分析に使われる
install.packages("gmodels")
# gmodelsパッケージをロード
library(gmodels)
color
View(usedcars)
usedcars$coloar
usedcars$color
summary(usedcars$color)
# 地味な色かそうでないかをダミー変数で定義
usedcars$consevative <- usedcars$color %in% c("Black", "Gray", "Silver", "White")
table (usedcars$consevative)
View(usedcars)
View(usedcars)
# 車の色についてクロステーブルの出力
CrossTable(x = usedcars$model, y = usedcars$consevative)
CrossTable(x = usedcars$model, y = usedcars$color)
# 車の色についてクロステーブルの出力
CrossTable(x = usedcars$model, y = usedcars$consevative)
# 車の色についてクロステーブルの出力
CrossTable(x = usedcars$model, y = usedcars$consevative, chisq = TRUE)
# 車の色についてクロステーブルの出力
CrossTable(x = usedcars$model, y = usedcars$consevative, chisq = TRUE)
q()
?dist
source('P:/Training/00_R/Training/3.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
# ウィスコンシン州乳がん診断データセットの読み込み
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
summary(wbcd)
str(wbcd)
source('P:/Training/00_R/Training/3.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
# IDは不要な情報であるため、IDを除いたデータセットを作成
wbcd <- wbcd[-1]
# データセットの内容の再確認
str(wbcd)
# 診断結果(diagnosis)を因子に変換し、ラベルもわかりやすいものに変換
wbcd$diagnosis <- factor(wbcd$diagnosis,
levels = c("B", "M"),
labels = c("Benign", "Malignant"))
# データセットの内容の再確認
str(wbcd)
diagnosis
wbcd$diagnosis
# 診断結果が想定通りのものになっているか確認
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
?normalize
??normalize
source('P:/Training/00_R/Training/3.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
normalize()
# normalize()関数が動作しているかを確認
normalize(c(1,2,3,4,5))
normalize(c(10,20,30,40,50))
?lapply
# normalize()関数を個々の要素に適用
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
?as.data.frame
is.data.frame(wbcd_n)
is.data.frame(wbcd)
is.data.frame(normalize())
is.data.frame(normalize
)
is.data.frame(wbcd_n)
summary(wbcd_n)
# 訓練用のデータセット(学習器)とシュミレート用のデータセットの作成
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
# 正規化された訓練データセットとテストデータセットに対し
# diagnosisのクラスラベルを格納
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
# k近傍法を実行するためのパッケージをインストール(初回のみ)
install.packages("class")
# classパッケージのロード
library(class)
?knn()
# knn()によるテストデータの分類
# kの値は、訓練データの総数である429の平方根に近い21に設定
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
class = wbcd_train_labels, k = 21)
# knn()によるテストデータの分類
# kの値は、訓練データの総数である429の平方根に近い21に設定
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k = 21)
# モデルの性能評価
library(gmodels)
# ベクトルの一致度を確認(カイ二乗検定の結果は出力しない)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)
prop.table(wbcd_test_labels$diagnosis)
prop.table(wbcd_test_labels)
table(wbcd_test_labels)
# モデルの性能改善
# Zスコア標準化を使用した距離計算の実行
wbcd_z <- as.data.frame(scale[wbcd[-1]])
# モデルの性能改善
# Zスコア標準化を使用した距離計算の実行
wbcd_z <- as.data.frame(scale(wbcd[-1]))
table wbcd_z
table(wbcd_z)
summary(wbcd_z)
# Zスコア標準化を使用したデータセットで訓練とシュミレーションの実行
wbcd.train_z <- wbcd_z[1:469, ]
remove wbcd.train_z
remove(wbcd.train_z)
# Zスコア標準化を使用したデータセットで訓練とシュミレーションの実行
wbcd_train_z <- wbcd_z[1:469, ]
wbcd_test_z <- wbcd_z[470:569, ]
wbcd_test_labels_z <- wbcd[1:469, 1]
rm(wbcd_test_labels_z)
wbcd_train_labels_z <- wbcd[1:469, 1]
wbcd_test_labels_z <- wbcd[470:569, 1]
wbcd_test_pred <- knn(train = wbcd_train_z, test = wbcd_test_z, cl = wbcd_train_labels_z, k = 21)
CrossTable(x = wbcd_test_labels_z, y = wbcd_test_pred, prop.chisq = FALSE)
CrossTable(x = wbcd_test_labels_z, y = wbcd_test_pred_z, prop.chisq = FALSE)
source('P:/Training/00_R/Training/3.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
5/39
q()
source('P:/Training/00_R/Training/3.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
source('P:/Training/00_R/Training/3.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
source('P:/Training/00_R/Training/3.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
source('P:/Training/00_R/Training/3.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
source('P:/Training/00_R/Training/3.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
source('P:/Training/00_R/Training/3.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
# ここまでに作成したデータセットのクリア
rm(list=ls())
# スパムメールのCSVデータを読み込み
sms_raw <- read.csv("sms_spam.csv", stringsAsFactors = FALSE)
str(sms_raw)
# SMS_RAWのSPAMとHAMが文字列データになっているので、因子に変換
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$type)
# tmパッケージ(テキストマイニング用のパッケージ)のインストール(初回のみ)
install.packages("tm")
# tmパッケージをロード
library(tm)
# tmパッケージ(テキストマイニング用のパッケージ)のインストール(初回のみ)
install.packages("tm")
# tmパッケージをロード
library(tm)
source('P:/Training/00_R/Training/4.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
# ここまでに作成したデータセットのクリア
rm(list=ls())
source('P:/Training/00_R/Training/4.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
remove("tm")
print(library(tm))
remove.packages("tm")
# tmパッケージ(テキストマイニング用のパッケージ)のインストール(初回のみ)
install.packages("tm")
# tmパッケージをロード
library(tm)
library()
remove.packages("tm")
install.packages("tm")
library("tm")
library(tm)
source('P:/Training/00_R/Training/4.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
.libPaths()
library(slam)
sms_raw <- VCorpus(VectorSource(sms_raw$text))
remove.packages("tm")
print(require("tm"))
print(require(tm))
print(require("tm"))
install.packages("tm")
print(require("tm"))
install.packages("knn")
install.packages("class")
print(require("class"))
library(class)
print(require("class"))
print(require("tm"))
install.packages("tm")
print(require("tm"))
library()
install.packages("randomForest")
library(randomForest)
print(require("randomforest"))
print(require("randomforest"))
library()
remove.packages("randomforest")
install.packages("cluster")
print(require("cluster"))
install.packages("rpart")
print(require("rpart"))
remove.packages("rpart")
print(require("rpart"))
remove.packages("rpart")
library(help="tm")
library(tm)
library("tm")
search()
install.packages("tm")
search()
library(tm)
remove.packages("tm")
install.packages("tm", verbose = TRUE)
?install.packages
list.files(.libPaths()[1])
remove.packages("tm")
list.files(.libPaths()[1])
install.packages("tm", verbose = TRUE)
libPath()
print(require("tm"))
library(tm)
install.packages("xlsx")
library(xlsx)
print(requ)
print(require("xlsx"))
list.files(.libPaths()[1])
install.packages(NULL, .libPaths()[1L], dependencies = NA, type = type)
1()
q()
# tmパッケージをロード
library(tm)
source('P:/Training/00_R/Training/4.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
install.packages("slam")
list.files
print(require("slam"))
instal.packages("slam")
install.packages("slam")
sessionInfo()
?setRepositories()
setRepositories()
setRepositories()
install.packages("slam")
ap <- available.packages()
view(ap)
View(ap)
"foobarbaz" %in% rownames(ap)
"sla" %in% rownames(ap)
"slam" %in% rownames(ap)
rversion
updateR()
install.packages("slam")
install.packages("tm")
library(tm)
print(require(tm))
install.packages("slam")
install.packages("slam")
library(tm)
print(require(slam))
remove.packages(tm)
remove.packages("tm")
source('P:/Training/00_R/Training/4.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
install.packages("tm")
# tmパッケージをロード
library(tm)
print(require("tm"))
?VectorSource
docs <- c("This is a text.", "This is another one")
(vs <- VectorSource(docs))
inspect(VCorpus(vs))
# コーパスオブジェクトを作成する
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
# コーパスの内容を確認し、訓練データに含まれる
# 5,559通のSMSメッセージが読み込まれているか確認する
print(sms_corpus)
packageVersion("tm")
inspect(sms_corpus[1:2])
inspect(sms_corpus[1:10])
inspect(sms_corpus[1:555])
inspect(sms_corpus[1:1000])
# コーパスの内容を小文字だけになるようにメッセージを標準化
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
# sms_corpus_cleanが正常に動作したか確認
as.character(sms_corpus[1])
as.character(sms_corpus_clean[1])
as.character(sms_corpus_clean[1])
# sms_corpus_cleanが正常に動作したか確認
as.character(sms_corpus[2])
as.character(sms_corpus_clean[2])
# sms_corpus_cleanが正常に動作したか確認
as.character(sms_corpus[153])
# sms_corpus_cleanが正常に動作したか確認
as.character(sms_corpus_clean[153])
# sms_corpus_cleanが正常に動作したか確認
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])
# SMSメッセージから数字を取り除く
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
?stopword
?stopwords
stopwords()
stopwords("german")
stopwords("SMART")
tm_map()
?tm_map()
q()
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
source('P:/Training/00_R/Training/4.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
install.packages("tm")
# tmパッケージをロード
library(tm)
# コーパスオブジェクトを作成する
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
# コーパスの内容を小文字だけになるようにメッセージを標準化
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
# sms_corpus_cleanが正常に動作したか確認
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])
# SMSメッセージから数字やstopwordsを取り除く
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
as.character(sms_corpus_clean[[1]])
removePunctuation("hello____world")
removePunctuation("hello.......world")
removePunctuation("hello...... fdjia .world")
removePunctuation("hello......--//.world")
removePunctuation("hello......--/  /.world")
removePunctuation("hello..... ..world")
?gsub
# 記号を空白に置換(テキストのコラムのやり方)
# 置換をするメソッドの作成
replacePunctuation <- function(x) {
gsb("[[:punct:]]+", " ", x)
}
remove replacePunctuation()
remove replacePunctuation
rm(replacePunctuation())
rm(replacePunctuation
)
# 記号を空白に置換(テキストのコラムのやり方)
# 置換をするメソッドの作成
replacePunctuation <- function(x) {
gsub("[[:punct:]]+", " ", x)
}
# 置換処理
sms_corpus_clean <- tm_map(sms_corpus_clean, replacePunctuation)
as.character((sms_corpus[1]))
as.character((sms_corpus[[1]))
as.character((sms_corpus[[1]]))
as.character((sms_corpus_clean[[1]]))
# stemming処理
# SnowballCパッケージのインストール
install.packages("SnowballC")
# stemming処理
# SnowballCパッケージのインストール(初回のみ)とロード
# install.packages("SnowballC")
library(SnowballC)
print(require(SnowballC))
wordStem(c("learn", "went", "distinguished", "haphazard"))
wordStem(c("learn", "learnt", "distinguished", "haphazard"))
wordStem(c("learn", "given", "distinguished", "haphazard"))
wordStem(c("learn", "learning", "distinguished", "learns"))
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
# 余分なスペースの削除
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
# 結果の確認
as.character(sms_corpus[1:3])
as.character(sms_corpus_clean[1:3])
# 結果の確認
as.character(sms_corpus[[1:3]])
as.character(sms_corpus_clean[1:3])
# 結果の確認
as.character(sms_corpus[1:3])
# 結果の確認
as.character(sms_corpus[[1:3]])
source('P:/Training/00_R/Training/4.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
install.packages("tm")
source('P:/Training/00_R/Training/4.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
install.packages("tm")
# tmパッケージをロード
library(tm)
# コーパスオブジェクトを作成する
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
# コーパスの内容を確認し、訓練データに含まれる
# 5,574通のSMSメッセージが読み込まれているか確認する
print(sms_corpus)
inspect(sms_corpus[1:1000])
# コーパスの内容を小文字だけになるようにメッセージを標準化
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
# sms_corpus_cleanが正常に動作したか確認
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])
# SMSメッセージから数字やstopwordsを取り除く
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
# 記号類を削除(テキストのやり方)
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
# 記号を空白に置換(テキストのコラムのやり方)
# 置換をするメソッドの作成
replacePunctuation <- function(x) {
gsub("[[:punct:]]+", " ", x)
}
# 置換処理
# sms_corpus_clean <- tm_map(sms_corpus_clean, replacePunctuation)
# stemming処理
# SnowballCパッケージのインストール(初回のみ)とロード
# install.packages("SnowballC")
library(SnowballC)
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
# 余分なスペースの削除
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
# 結果の確認
as.character(sms_corpus[1:3])
as.character(sms_corpus_clean[1:3])
as.character(sms_corpus[[1:3]])
# DTM(Document-Term Matorix：文書索引語行列)の作成
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
table(sms_dtm)
sms_dtm
# 前処理を行っていない場合の対応方法は以下の通り
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(
tolower = TRUE,
removeNumbers = TRUE,
stopwords = TRUE,
removePunctuation = TRUE,
stemming = TRUE
))
sms_dtm2
sms_corpus
sms_dtm_train <- sms_dtm[1:4174]
sms_dtm_test <- sms_dtm[4175:5574]
sms_dtm_train <- sms_dtm[1:4174,]$type
sms_dtm_test <- sms_dtm[4175:5574,]$type
rm(sms_dtm_test, sms_dtm_train)
sms_dtm_train <- sms_dtm[1:4174,]
sms_dtm_test <- sms_dtm[4175:5574,]
sms_train_labels <- sms_raw[1:4174,]$type
sms_test_labels <- sms_raw[4175:5574,]$type
# 訓練データのラベルとテストデータのラベルの違いを確認
# ※それほど違いが大きくなければ問題ない
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
# wordcloudを実行するためのパッケージを準備
install.packages("wordcloud")
library(wordcloud)
print(require("wordcloud"))
# 全体のワードクラウド
wordcloud(sms_corpus_clean, mn.freq = 50, random.order = FALSE)
warning()
warnings()
wordcloud(sms_corpus_clean, mn.freq = 50, random.order = FALSE)
warnings()
# 全体のワードクラウド
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
wordcloud(sms_corpus_clean, min.freq = 50)
wordcloud(sms_corpus_clean, min.freq = 50)
wordcloud(sms_corpus_clean, min.freq = 10, random.order = FALSE)
wordcloud(sms_corpus_clean, min.freq = 100, random.order = FALSE)
# spamとhamのメッセージのサブセットを作成
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type == "ham")
# spamとhamのワードクラウドの作成
wordcloud(spam$text, max.words = 40, random.order = FALSE)
wordcloud(ham$text, max.words = 40, random.order = FALSE)
# 単純ベイズ法で使用する頻出語を確認
# 出現回数が少ない単語を削除
findFreqTerms(sms_dtm_train, 5)
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
sms_freq_words
table(sms_freq_words)
sms_dtm_freq_train <- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
str(sms_dtm_freq_test)
str(sms_dtm_freq_train)
sms_dtm_freq_train
sms_dtm_freq_test
sms_freq_words
str(sms_freq_words)
sms_dtm_freq_train
str(sms_dtm_freq_train)
# 各文に頻出単語が含まれるかどうかをカテゴリで示す
convert_count <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
# 各文に頻出単語が含まれるかどうかをカテゴリで示す
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
rm (convert_count)
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
str(sms_train)
str(sms_test)
# 単純ベイズ実装が含まれるパッケージのインストール
install.packages("e1071")
library(e1071)
print(require("e1071"))
# 単純ベイズのモデル構築
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
# 単純ベイズモデルの性能評価
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
install.packages("gmodels")
library(gmodels)
CrossTable(sms_test_pred, sms_test_labels,)
CrossTable(sms_test_pred, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
# 単純ベイズモデルにラプラス推定量を追加した場合
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 1)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
source('P:/Training/00_R/Training/4.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
source('P:/Training/00_R/Training/4.2_Textbook.R', encoding = 'UTF-8', echo=TRUE)
